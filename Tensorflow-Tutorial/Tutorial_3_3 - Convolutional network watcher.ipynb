{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CNN 网络结构查看\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.contrib.layers.python.layers import initializers\n",
    "\n",
    "# 设置按需使用GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_weight_initializer = initializers.xavier_initializer()\n",
    "conv_biases_initializer = init_ops.zeros_initializer()\n",
    "\n",
    "fc_weight_initializer = init_ops.truncated_normal_initializer(0.0, 0.005)\n",
    "fc_biases_initializer = init_ops.constant_initializer(0.1)\n",
    "\n",
    "keep_prob = 0.5\n",
    "\n",
    "def conv2d(scope, x, filter_shape, strides_x, strides_y, padding, weights_initializer=None,\n",
    "           biases_initializer=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        scope: scope name of this layer.\n",
    "        x: 4-D inputs. [batch_size, in_height, in_width, in_channels]\n",
    "        filter_shape: A list of ints.[filter_height, filter_width, in_channels, out_channels]\n",
    "        strides: A list of ints. 1-D tensor of length 4. The stride of the sliding window for each dimension of input.\n",
    "        padding: A string from: \"SAME\", \"VALID\". The type of padding algorithm to use.\n",
    "    Returns:\n",
    "        h_conv:  A 4-D tensor. [batch_size, out_height, out_width, out_channels].\n",
    "        if padding is 'SAME', then out_height==in_height.\n",
    "        else, out_height = in_height - filter_height + 1.\n",
    "        the same for out_width.\n",
    "    \"\"\"\n",
    "    assert padding in ['SAME', 'VALID']\n",
    "    if weights_initializer is None:\n",
    "        weights_initializer = conv_weight_initializer\n",
    "    if biases_initializer is None:\n",
    "        biases_initializer = conv_biases_initializer\n",
    "    strides = [1, strides_x, strides_y, 1]\n",
    "    with tf.variable_scope(scope):\n",
    "        W_conv = tf.get_variable('w', shape=filter_shape, initializer=weights_initializer)\n",
    "        b_conv = tf.get_variable('b', shape=[filter_shape[-1]], initializer=biases_initializer)\n",
    "        h_conv = tf.nn.conv2d(x, W_conv, strides=strides, padding=padding)\n",
    "        h_conv_relu = tf.nn.relu(h_conv + b_conv)\n",
    "    return h_conv_relu\n",
    "\n",
    "def max_pooling(scope, x, k_height, k_width, strides_x, strides_y, padding='SAME'):\n",
    "    \"\"\"max pooling layer.\"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        ksize = [1, k_height, k_width, 1]\n",
    "        strides = [1, strides_x, strides_y, 1]\n",
    "        h_pool = tf.nn.max_pool(x, ksize, strides, padding)\n",
    "    return h_pool\n",
    "\n",
    "\n",
    "def dropout(x, keep_prob, name=None):\n",
    "    \"\"\"dropout layer\"\"\"\n",
    "    return tf.nn.dropout(x, keep_prob, name=name)\n",
    "\n",
    "def fc(name, x, in_size, out_size, weights_initializer=None, biases_initializer=None, activation=None):\n",
    "    \"\"\"fully-connect\n",
    "    Args:\n",
    "        x: 2-D tensor, [batch_size, in_size]\n",
    "        in_size: the size of input tensor.\n",
    "        out_size: the size of output tensor.\n",
    "        activation: 'relu' or 'sigmoid' or 'tanh'.\n",
    "    Returns:\n",
    "        h_fc: 2-D tensor, [batch_size, out_size].\n",
    "    \"\"\"\n",
    "    if activation is not None:\n",
    "        assert activation in ['relu', 'sigmoid', 'tanh'], 'Wrong activation function.'\n",
    "    if weights_initializer is None:\n",
    "        weights_initializer = fc_weight_initializer\n",
    "    if biases_initializer is None:\n",
    "        biases_initializer = fc_biases_initializer\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', shape=[in_size, out_size], initializer=weights_initializer)\n",
    "        b = tf.get_variable('b', [out_size], initializer=biases_initializer)\n",
    "        h_fc = tf.nn.xw_plus_b(x, w, b)\n",
    "        if activation == 'relu':\n",
    "            return tf.nn.relu(h_fc)\n",
    "        elif activation == 'tanh':\n",
    "            return tf.nn.tanh(h_fc)\n",
    "        elif activation == 'sigmoid':\n",
    "            return tf.nn.sigmoid(h_fc)\n",
    "        else:\n",
    "            return h_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_X_inputs = tf.placeholder(tf.float32, [None, 224, 224, 3], name='X_inputs')\n",
    "_y_inputs = tf.placeholder(tf.int64, [None], name='y_input')\n",
    "\n",
    "# build the model\n",
    "with tf.variable_scope('cnn'):\n",
    "    # 1st Layer: Conv (w ReLu) -> Lrn -> Pool\n",
    "    conv1 = conv2d('conv1', _X_inputs, [11, 11, 3, 64], 4, 4, 'VALID')\n",
    "    pool1 = max_pooling('pool1', conv1, 3, 3, 2, 2, 'VALID')\n",
    "\n",
    "    # 2nd Layer: Conv (w ReLu)  -> Lrn -> Pool with 2 groups\n",
    "    conv2 = conv2d('conv2', pool1, [5, 5, 64, 192], 1, 1, 'SAME')\n",
    "    pool2 = max_pooling('pool2', conv2, 3, 3, 2, 2, 'VALID')\n",
    "\n",
    "    # 3rd Layer: Conv (w ReLu)\n",
    "    conv3 = conv2d('conv3', pool2, [3, 3, 192, 384], 1, 1, 'SAME')\n",
    "\n",
    "    # 4th Layer: Conv (w ReLu)\n",
    "    conv4 = conv2d('conv4', conv3, [3, 3, 384, 384], 1, 1, 'SAME')\n",
    "\n",
    "    # 5th Layer: Conv (w ReLu) -> Pool splitted into two groups\n",
    "    conv5 = conv2d('conv5', conv4, [3, 3, 384, 256], 1, 1, 'SAME')\n",
    "    pool5 = max_pooling('pool5', conv5, 3, 3, 2, 2, 'VALID')\n",
    "\n",
    "    # 6th Layer: Flatten -> FC (w ReLu) -> Dropout\n",
    "    fc6 = conv2d('fc6', pool5, [5, 5, 256, 4096], 1, 1, 'VALID',\n",
    "                           weights_initializer=fc_weight_initializer,\n",
    "                           biases_initializer=fc_biases_initializer)\n",
    "    dropout6 = dropout(fc6, keep_prob)\n",
    "\n",
    "    # 7th Layer: FC (w ReLu) -> Dropout\n",
    "    fc7 = conv2d('fc7', dropout6, [1, 1, 4096, 4096], 1, 1, 'VALID',\n",
    "                           weights_initializer=fc_weight_initializer,\n",
    "                           biases_initializer=fc_biases_initializer)\n",
    "    dropout7 = dropout(fc7, keep_prob)\n",
    "\n",
    "    # 8th Layer: FC and return unscaled activations\n",
    "    flattened = tf.squeeze(dropout7, [1, 2], name='fc8/squeezed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"X_inputs:0\", shape=(?, 224, 224, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(_X_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"cnn/conv1/Relu:0\", shape=(?, 54, 54, 64), dtype=float32)\n",
      "Tensor(\"cnn/pool1/MaxPool:0\", shape=(?, 26, 26, 64), dtype=float32)\n",
      "Tensor(\"cnn/conv2/Relu:0\", shape=(?, 26, 26, 192), dtype=float32)\n",
      "Tensor(\"cnn/pool2/MaxPool:0\", shape=(?, 12, 12, 192), dtype=float32)\n",
      "Tensor(\"cnn/conv3/Relu:0\", shape=(?, 12, 12, 384), dtype=float32)\n",
      "Tensor(\"cnn/conv4/Relu:0\", shape=(?, 12, 12, 384), dtype=float32)\n",
      "Tensor(\"cnn/conv5/Relu:0\", shape=(?, 12, 12, 256), dtype=float32)\n",
      "Tensor(\"cnn/pool5/MaxPool:0\", shape=(?, 5, 5, 256), dtype=float32)\n",
      "Tensor(\"cnn/fc6/Relu:0\", shape=(?, 1, 1, 4096), dtype=float32)\n",
      "Tensor(\"cnn/fc7/Relu:0\", shape=(?, 1, 1, 4096), dtype=float32)\n",
      "Tensor(\"cnn/fc8/squeezed:0\", shape=(?, 4096), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(conv1)\n",
    "print(pool1)\n",
    "print(conv2)\n",
    "print(pool2)\n",
    "print(conv3)\n",
    "print(conv4)\n",
    "print(conv5)\n",
    "print(pool5)\n",
    "print(fc6)\n",
    "print(fc7)\n",
    "print(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# alexnet v2\n",
    "conv1 = conv2d('conv1', X, [11, 11, 3, 64], 4, 4, 'VALID')\n",
    "print(conv1)\n",
    "pool1 = max_pooling(conv1, 3, 3, 2, 2, 'VALID')\n",
    "print(pool1)\n",
    "\n",
    "conv2 = conv2d('conv2', pool1, [5, 5, 64, 192], 1, 1, 'SAME')\n",
    "print(conv2)\n",
    "pool2 = max_pooling(conv2, 3, 3, 2, 2, 'VALID')\n",
    "print(pool2)\n",
    "\n",
    "conv3 = conv2d('conv3', pool2, [3, 3, 192, 384], 1, 1, 'SAME')\n",
    "print(conv3)\n",
    "conv4 = conv2d('conv4', conv3, [3, 3, 384, 384], 1, 1, 'SAME')\n",
    "print(conv4)\n",
    "conv5 = conv2d('conv5', conv4, [3, 3, 384, 256], 1, 1, 'SAME')\n",
    "print(conv5)\n",
    "pool5 = max_pooling(conv5, 3, 3, 2, 2, 'VALID')\n",
    "print(pool5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc6 = conv2d('fc6', pool5, [5, 5, 256, 4096], 1, 1, 'VALID')\n",
    "print(fc6)\n",
    "drop6 = dropout(fc6, 0.5)\n",
    "print(drop6)\n",
    "fc7 = conv2d('fc7', drop6, [1, 1, 4096, 4096], 1, 1, 'SAME')\n",
    "print(fc7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.contrib.layers.python.layers import initializers\n",
    "\n",
    "\"\"\" img_6_sketch_a_net\n",
    "Sketch-a-Net: a Deep Neural Network that Beats Humans\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Settings(object):\n",
    "    def __init__(self):\n",
    "        self.model_name = 'img_6_sketch_a_net'\n",
    "        self.summary_path = '../../summary/' + self.model_name + '/'\n",
    "        self.ckpt_path = '../../ckpt/' + self.model_name + '/'\n",
    "        self.img_size = 225\n",
    "        self.n_channel = 1\n",
    "        self.n_class = 345\n",
    "        self.l2_weight_decay = 0.0005\n",
    "\n",
    "\n",
    "class CNN(object):\n",
    "    \"\"\"\n",
    "    CNN: X_inputs=[batch_size, 224, 224, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, settings):\n",
    "        self.model_name = settings.model_name\n",
    "        self.img_size = settings.img_size\n",
    "        self.n_channel = settings.n_channel\n",
    "        self.n_class = settings.n_class\n",
    "        self.l2_weight_decay = settings.l2_weight_decay\n",
    "        self._global_step = tf.Variable(0, trainable=False, name='Global_Step')\n",
    "        # placeholders\n",
    "        self._keep_prob = tf.placeholder(tf.float32, [])\n",
    "        self._batch_size = tf.placeholder(tf.int32, [])\n",
    "\n",
    "        self.conv_weight_initializer = initializers.xavier_initializer()\n",
    "        self.conv_biases_initializer = init_ops.zeros_initializer()\n",
    "\n",
    "        self.fc_weight_initializer = init_ops.truncated_normal_initializer(0.0, 0.005)\n",
    "        self.fc_biases_initializer = init_ops.constant_initializer(0.1)\n",
    "\n",
    "        with tf.name_scope('Inputs'):\n",
    "            self._X_inputs = tf.placeholder(tf.float32, [None, self.img_size, self.img_size, self.n_channel],\n",
    "                                            name='X_inputs')\n",
    "            self._y_inputs = tf.placeholder(tf.int64, [None], name='y_input')\n",
    "\n",
    "        # build the model\n",
    "        with tf.variable_scope('cnn'):\n",
    "            # 1st Layer: Conv (w ReLu) -> Lrn -> Pool\n",
    "            self.conv1 = self.conv2d('conv1', self._X_inputs, [15, 15, 1, 64], 3, 3, 'VALID')\n",
    "            self.pool1 = self.max_pooling('pool1', self.conv1, 3, 3, 2, 2, 'VALID')\n",
    "            print(self.conv1)\n",
    "            print(self.pool1)\n",
    "\n",
    "            # 2nd Layer: Conv (w ReLu)  -> Lrn -> Pool with 2 groups\n",
    "            self.conv2 = self.conv2d('conv2', self.pool1, [5, 5, 64, 128], 1, 1, 'SAME')\n",
    "            self.pool2 = self.max_pooling('pool2', self.conv2, 3, 3, 2, 2, 'VALID')\n",
    "            print(self.conv2)\n",
    "            print(self.pool2)\n",
    "\n",
    "            # 3rd Layer: Conv (w ReLu)\n",
    "            self.conv3 = self.conv2d('conv3', self.pool2, [3, 3, 128, 256], 1, 1, 'SAME')\n",
    "\n",
    "            # 4th Layer: Conv (w ReLu)\n",
    "            self.conv4 = self.conv2d('conv4', self.conv3, [3, 3, 256, 256], 1, 1, 'SAME')\n",
    "\n",
    "            # 5th Layer: Conv (w ReLu) -> Pool splitted into two groups\n",
    "            self.conv5 = self.conv2d('conv5', self.conv4, [3, 3, 256, 256], 1, 1, 'SAME')\n",
    "            self.pool5 = self.max_pooling('pool5', self.conv5, 3, 3, 2, 2, 'VALID')\n",
    "            print(self.conv3)\n",
    "            print(self.conv4)\n",
    "            print(self.conv5)\n",
    "            print(self.pool5)\n",
    "\n",
    "            # 6th Layer: Flatten -> FC (w ReLu) -> Dropout\n",
    "            self.fc6 = self.conv2d('fc6', self.pool5, [7, 7, 256, 512], 1, 1, 'VALID',\n",
    "                                   weights_initializer=self.fc_weight_initializer,\n",
    "                                   biases_initializer=self.fc_biases_initializer)\n",
    "            self.dropout6 = self.dropout(self.fc6, self.keep_prob)\n",
    "\n",
    "            # 7th Layer: FC (w ReLu) -> Dropout\n",
    "            self.fc7 = self.conv2d('fc7', self.dropout6, [1, 1, 512, 512], 1, 1, 'VALID',\n",
    "                                   weights_initializer=self.fc_weight_initializer,\n",
    "                                   biases_initializer=self.fc_biases_initializer)\n",
    "            self.dropout7 = self.dropout(self.fc7, self.keep_prob)\n",
    "\n",
    "            # 8th Layer: FC and return unscaled activations\n",
    "            self.flattened = tf.squeeze(self.dropout7, [1, 2], name='fc8/squeezed')\n",
    "            self._y_pred = self.fc8 = self.fc('fc8', self.flattened, 512, self.n_class,\n",
    "                                              biases_initializer=init_ops.zeros_initializer())\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            with tf.name_scope('softmax_loss'):\n",
    "                self._softmax_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self._y_pred, labels=self._y_inputs))\n",
    "\n",
    "            with tf.name_scope('l2_loss'):\n",
    "                self._l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'weights/' in v.name])\n",
    "\n",
    "            with tf.name_scope('total_loss'):\n",
    "                self._total_loss = self._softmax_loss + self._l2_loss * self.l2_weight_decay\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            self._correct_prediction = tf.equal(tf.argmax(self._y_pred, 1), self._y_inputs)\n",
    "            self._accuracy = tf.reduce_mean(tf.cast(self._correct_prediction, \"float\"))\n",
    "\n",
    "        self.saver = tf.train.Saver(max_to_keep=30)\n",
    "\n",
    "    @property\n",
    "    def keep_prob(self):\n",
    "        return self._keep_prob\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def global_step(self):\n",
    "        return self._global_step\n",
    "\n",
    "    @property\n",
    "    def X_inputs(self):\n",
    "        return self._X_inputs\n",
    "\n",
    "    @property\n",
    "    def y_inputs(self):\n",
    "        return self._y_inputs\n",
    "\n",
    "    @property\n",
    "    def y_pred(self):\n",
    "        return self._y_pred\n",
    "\n",
    "    @property\n",
    "    def soft_loss(self):\n",
    "        return self._softmax_loss\n",
    "\n",
    "    @property\n",
    "    def l2_loss(self):\n",
    "        return self._l2_loss\n",
    "\n",
    "    @property\n",
    "    def total_loss(self):\n",
    "        return self._total_loss\n",
    "\n",
    "    @property\n",
    "    def correct_prediction(self):\n",
    "        return self._correct_prediction\n",
    "\n",
    "    @property\n",
    "    def accuracy(self):\n",
    "        return self._accuracy\n",
    "\n",
    "    def conv2d(self, scope, x, filter_shape, strides_x, strides_y, padding, weights_initializer=None,\n",
    "               biases_initializer=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            scope: scope name of this layer.\n",
    "            x: 4-D inputs. [batch_size, in_height, in_width, in_channels]\n",
    "            filter_shape: A list of ints.[filter_height, filter_width, in_channels, out_channels]\n",
    "            strides: A list of ints. 1-D tensor of length 4. The stride of the sliding window for each dimension of input.\n",
    "            padding: A string from: \"SAME\", \"VALID\". The type of padding algorithm to use.\n",
    "        Returns:\n",
    "            h_conv:  A 4-D tensor. [batch_size, out_height, out_width, out_channels].\n",
    "            if padding is 'SAME', then out_height==in_height.\n",
    "            else, out_height = in_height - filter_height + 1.\n",
    "            the same for out_width.\n",
    "        \"\"\"\n",
    "        assert padding in ['SAME', 'VALID']\n",
    "        if weights_initializer is None:\n",
    "            weights_initializer = self.conv_weight_initializer\n",
    "        if biases_initializer is None:\n",
    "            biases_initializer = self.conv_biases_initializer\n",
    "        strides = [1, strides_x, strides_y, 1]\n",
    "        with tf.variable_scope(scope):\n",
    "            W_conv = tf.get_variable('weights', shape=filter_shape, initializer=weights_initializer)\n",
    "            b_conv = tf.get_variable('biases', shape=[filter_shape[-1]], initializer=biases_initializer)\n",
    "            h_conv = tf.nn.conv2d(x, W_conv, strides=strides, padding=padding)\n",
    "            h_conv_relu = tf.nn.relu(h_conv + b_conv)\n",
    "        return h_conv_relu\n",
    "\n",
    "    @staticmethod\n",
    "    def max_pooling(scope, x, k_height, k_width, strides_x, strides_y, padding='SAME'):\n",
    "        \"\"\"max pooling layer.\"\"\"\n",
    "        with tf.variable_scope(scope):\n",
    "            ksize = [1, k_height, k_width, 1]\n",
    "            strides = [1, strides_x, strides_y, 1]\n",
    "            h_pool = tf.nn.max_pool(x, ksize, strides, padding)\n",
    "        return h_pool\n",
    "\n",
    "    @staticmethod\n",
    "    def dropout(x, keep_prob, name=None):\n",
    "        \"\"\"dropout layer\"\"\"\n",
    "        return tf.nn.dropout(x, keep_prob, name=name)\n",
    "\n",
    "    def fc(self, name, x, in_size, out_size, weights_initializer=None, biases_initializer=None, activation=None):\n",
    "        \"\"\"fully-connect\n",
    "        Args:\n",
    "            x: 2-D tensor, [batch_size, in_size]\n",
    "            in_size: the size of input tensor.\n",
    "            out_size: the size of output tensor.\n",
    "            activation: 'relu' or 'sigmoid' or 'tanh'.\n",
    "        Returns:\n",
    "            h_fc: 2-D tensor, [batch_size, out_size].\n",
    "        \"\"\"\n",
    "        if activation is not None:\n",
    "            assert activation in ['relu', 'sigmoid', 'tanh'], 'Wrong activation function.'\n",
    "        if weights_initializer is None:\n",
    "            weights_initializer = self.fc_weight_initializer\n",
    "        if biases_initializer is None:\n",
    "            biases_initializer = self.fc_biases_initializer\n",
    "        with tf.variable_scope(name):\n",
    "            w = tf.get_variable('weights', shape=[in_size, out_size], initializer=weights_initializer)\n",
    "            b = tf.get_variable('biases', [out_size], initializer=biases_initializer)\n",
    "            h_fc = tf.nn.xw_plus_b(x, w, b)\n",
    "            if activation == 'relu':\n",
    "                return tf.nn.relu(h_fc)\n",
    "            elif activation == 'tanh':\n",
    "                return tf.nn.tanh(h_fc)\n",
    "            elif activation == 'sigmoid':\n",
    "                return tf.nn.sigmoid(h_fc)\n",
    "            else:\n",
    "                return h_fc\n",
    "\n",
    "# # test the model\n",
    "# def test():\n",
    "#     import numpy as np\n",
    "#     print('Begin testing...')\n",
    "#     settings = Settings()\n",
    "#     config = tf.ConfigProto()\n",
    "#     config.gpu_options.allow_growth = True\n",
    "#     batch_size = 128\n",
    "#     with tf.Session(config=config) as sess:\n",
    "#         model = CNN(settings)\n",
    "#         optimizer = tf.train.AdamOptimizer(0.001)\n",
    "#         train_op = optimizer.minimize(model.soft_loss)\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "#         fetch = [model.soft_loss, model.accuracy, model.y_pred, train_op]\n",
    "#         loss_list = list()\n",
    "#         for i in xrange(100):\n",
    "#             X_batch = np.zeros((batch_size, 224, 224, 3), dtype=float)\n",
    "#             y_batch = np.zeros(batch_size, dtype=int)\n",
    "#             _batch_size = len(y_batch)\n",
    "#             feed_dict = {model.X_inputs: X_batch, model.y_inputs: y_batch,\n",
    "#                          model.batch_size: _batch_size, model.keep_prob: 0.5}\n",
    "#             loss, acc, y_pred, _ = sess.run(fetch, feed_dict=feed_dict)\n",
    "#             loss_list.append(loss)\n",
    "#             print(i, loss, acc)\n",
    "#\n",
    "#\n",
    "# if __name__ == '__main__':\n",
    "#     test()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
