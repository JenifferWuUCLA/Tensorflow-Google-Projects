{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CNN做MNIST分类\n",
    "refer: https://github.com/hjptriplebee/AlexNet_with_tensorflow/blob/master/alexnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 设置按需使用GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.导入数据，用 tensorflow 导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(10000, 10)\n",
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 用tensorflow 导入数据\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "# 看看咱们样本的数量\n",
    "print mnist.test.labels.shape\n",
    "print mnist.train.labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building network.\n"
     ]
    }
   ],
   "source": [
    "# 定义卷积层\n",
    "def conv2d(x, filter_shape, strides_x, strides_y, padding, name):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        x: 4-D inputs. [batch_size, in_height, in_width, in_channels]\n",
    "        filter_shape: A list of ints.[filter_height, filter_width, in_channels, out_channels]\n",
    "        strides: A list of ints. 1-D tensor of length 4. The stride of the sliding window for each dimension of input.\n",
    "        padding: A string from: \"SAME\", \"VALID\". The type of padding algorithm to use.\n",
    "    Returns:\n",
    "        h_conv:  A 4-D tensor. [batch_size, out_height, out_width, out_channels]. \n",
    "        if padding is 'SAME', then out_height==in_height. \n",
    "        else, out_height = in_height - filter_height + 1.\n",
    "        the same for out_width.\n",
    "    \"\"\"\n",
    "    assert padding in ['SAME', 'VALID']\n",
    "    strides=[1,strides_x, strides_y,1]\n",
    "    with tf.variable_scope(name):\n",
    "        W_conv = tf.get_variable('w', shape=filter_shape)\n",
    "        b_conv = tf.get_variable('b', shape=[filter_shape[-1]])\n",
    "        h_conv = tf.nn.conv2d(x, W_conv, strides=strides, padding=padding)\n",
    "        h_conv_relu = tf.nn.relu(h_conv + b_conv)\n",
    "    return h_conv_relu\n",
    "    \n",
    "\n",
    "def max_pooling(x, k_height, k_width, strides_x, strides_y, padding='SAME'):\n",
    "    \"\"\"max pooling layer.\"\"\"\n",
    "    ksize=[1,k_height, k_width,1]\n",
    "    strides=[1,strides_x, strides_y,1]\n",
    "    h_pool = tf.nn.max_pool(x, ksize, strides, padding)\n",
    "    return h_pool\n",
    "\n",
    "def dropout(x, keep_prob, name=None):\n",
    "    \"\"\"dropout layer\"\"\"\n",
    "    return tf.nn.dropout(x, keep_prob, name=name)\n",
    "\n",
    "def fc(x, in_size, out_size, name, activation=None):\n",
    "    \"\"\"fully-connect\n",
    "    Args:\n",
    "        x: 2-D tensor, [batch_size, in_size]\n",
    "        in_size: the size of input tensor.\n",
    "        out_size: the size of output tensor.\n",
    "        activation: 'relu' or 'sigmoid' or 'tanh'.\n",
    "    Returns:\n",
    "        h_fc: 2-D tensor, [batch_size, out_size].\n",
    "    \"\"\"\n",
    "    if activation is not None:\n",
    "        assert activation in ['relu', 'sigmoid', 'tanh'], 'Wrong activation function.'\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', shape = [in_size, out_size], dtype=tf.float32)\n",
    "        b = tf.get_variable('b', [out_size], dtype=tf.float32)\n",
    "        h_fc = tf.nn.xw_plus_b(x, w, b)\n",
    "        if activation == 'relu':\n",
    "            return tf.nn.relu(h_fc)\n",
    "        elif activation == 'tanh':\n",
    "            return tf.nn.tanh(h_fc)\n",
    "        elif activation == 'sigmoid':\n",
    "            return tf.nn.sigmoid(h_fc)\n",
    "        else:\n",
    "            return h_fc\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    X_ = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# 把X转为卷积所需要的形式\n",
    "X = tf.reshape(X_, [-1, 28, 28, 1])\n",
    "h_conv1 = conv2d(X, [5, 5, 1, 32], 1, 1, 'SAME', 'conv1')\n",
    "h_pool1 = max_pooling(h_conv1, 2, 2, 2, 2)\n",
    "\n",
    "h_conv2 = conv2d(h_pool1, [5, 5, 32, 64], 1, 1, 'SAME', 'conv2')\n",
    "h_pool2 = max_pooling(h_conv2, 2, 2, 2, 2)\n",
    "\n",
    "# flatten\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = fc(h_pool2_flat, 7*7*64, 1024, 'fc1', 'relu')\n",
    "\n",
    "# dropout: 输出的维度和h_fc1一样，只是随机部分值被值为零\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "h_fc2 = fc(h_fc1_drop, 1024, 10, 'fc2')\n",
    "y_conv = tf.nn.softmax(h_fc2)\n",
    "print('Finished building network.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.训练和评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 在测试的时候不使用 mini_batch， 那么测试的时候会占用较多的GPU（4497M），这在 notebook 交互式编程中是不推荐的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.02\n",
      "step 1000, training accuracy 1\n",
      "step 2000, training accuracy 0.98\n",
      "step 3000, training accuracy 0.98\n",
      "step 4000, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 8000, training accuracy 0.96\n",
      "step 9000, training accuracy 1\n",
      "test accuracy 0.9912\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(10000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%1000 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            X_:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print \"step %d, training accuracy %g\"%(i, train_accuracy)\n",
    "    train_step.run(feed_dict={X_: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print \"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    X_: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 下面改成了 test 也用 mini_batch 的形式， 显存只用了 529M,所以还是很成功的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 题外话：在做这个例子的过程中遇到过：资源耗尽的错误，为什么？\n",
    "# -> 因为之前每次做 train_acc  的时候用了全部的 55000 张图，显存爆了.\n",
    "\n",
    "# 1.损失函数：cross_entropy\n",
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))\n",
    "# 2.优化函数：AdamOptimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# 3.预测准确结果统计\n",
    "#　预测值中最大值（１）即分类结果，是否等于原始标签中的（１）的位置。argmax()取最大值所在的下标\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.arg_max(y_, 1))  \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# 如果一次性来做测试的话，可能占用的显存会比较多，所以测试的时候也可以设置较小的batch来看准确率\n",
    "test_acc_sum = tf.Variable(0.0)\n",
    "batch_acc = tf.placeholder(tf.float32)\n",
    "new_test_acc_sum = tf.add(test_acc_sum, batch_acc)\n",
    "update = tf.assign(test_acc_sum, new_test_acc_sum)\n",
    "\n",
    "# 定义了变量必须要初始化，或者下面形式\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# 或者某个变量单独初始化 如：\n",
    "# x.initializer.run()\n",
    "\n",
    "# 训练\n",
    "for i in range(5000):\n",
    "    X_batch, y_batch = mnist.train.next_batch(batch_size=50)\n",
    "    if i % 500 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={X_: X_batch, y_: y_batch, keep_prob: 1.0})\n",
    "        print \"step %d, training acc %g\" % (i, train_accuracy)\n",
    "    train_step.run(feed_dict={X_: X_batch, y_: y_batch, keep_prob: 0.5})  \n",
    "\n",
    "# 全部训练完了再做测试，batch_size=100\n",
    "for i in range(100): \n",
    "    X_batch, y_batch = mnist.test.next_batch(batch_size=100)\n",
    "    test_acc = accuracy.eval(feed_dict={X_: X_batch, y_: y_batch, keep_prob: 1.0})\n",
    "    update.eval(feed_dict={batch_acc: test_acc})\n",
    "    if (i+1) % 20 == 0:\n",
    "        print \"testing step %d, test_acc_sum %g\" % (i+1, test_acc_sum.eval())\n",
    "print \" test_accuracy %g\" % (test_acc_sum.eval() / 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 查看网络中间结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 在学习 CNN 的过程中，老是看到他们用图片的形式展示了中间层卷积的输出。好吧，这下我必须得自己实现以下看看呀！！！</b>\n",
    "<br/> 关于 python 图片操作主要有 matplotlib 和 PIL 两个库（refer to: http://www.cnblogs.com/yinxiangnan-charles/p/5928689.html）。\n",
    "<br/>我们使用 matplotlib 来完成这个任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 图像操作基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 我们先来看看数据是什么样的\n",
    "img1 = mnist.train.images[1]\n",
    "label1 = mnist.train.labels[1]\n",
    "print label1  # 所以这个是数字 6 的图片\n",
    "print 'img_data shape =', img1.shape  # 我们需要把它转为 28 * 28 的矩阵\n",
    "img1.shape = [28, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg  # 用于读取图片，这里用不上\n",
    "\n",
    "print img1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(img1)\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好吧，是显示了图片，但是结果是热度图像。我们想显示的是灰度图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 我们可以通过设置 cmap 参数来显示灰度图\n",
    "plt.imshow(img1, cmap='gray') # 'hot' 是热度图\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们想看 Conv1 层的32个卷积滤波后的结果，显示在同一张图上。 python 中也有 plt.subplot(121) 这样的方法来帮我们解决这个问题。如下：先看两个试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(4,8,1)\n",
    "plt.imshow(img1, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(4,8,2)\n",
    "plt.imshow(img1, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 显示网络中间结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，有了前面的图像操作基础，我们就该试试吧！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 首先应该把 img1 转为正确的shape (None, 784)\n",
    "X_img = img1.reshape([-1, 784])\n",
    "y_img = mnist.train.labels[1].reshape([-1, 10])\n",
    "# 我们要看 Conv1 的结果，即 h_conv1\n",
    "result = h_conv1.eval(feed_dict={X_: X_img, y_: y_img, keep_prob: 1.0})\n",
    "print result.shape\n",
    "print type(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的，我们成功的计算得到了 h_conv1，那么赶紧 imshow() 看看吧！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for _ in xrange(32):\n",
    "    show_img = result[:,:,:,_]\n",
    "    show_img.shape = [28, 28]\n",
    "    plt.subplot(4, 8, _ + 1)\n",
    "    plt.imshow(show_img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "哈哈，成功啦！从上面的结果中，我们可以看到不同的滤波器（卷积核）学习到了不同的特征。比如第一行中，第一个滤波器学习到了边缘信息，第5个卷积核，则学习到了骨干的信息。感觉好有趣，不由自主的想对另一个数字看看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 输出前10个看看，我选择数字 9 来试试\n",
    "print mnist.train.labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 首先应该把 img1 转为正确的shape (None, 784)\n",
    "X_img = mnist.train.images[2].reshape([-1, 784])\n",
    "y_img = mnist.train.labels[1].reshape([-1, 10]) # 这个标签只要维度一致就行了\n",
    "result = h_conv1.eval(feed_dict={X_: X_img, y_: y_img, keep_prob: 1.0})\n",
    "\n",
    "for _ in xrange(32):\n",
    "    show_img = result[:,:,:,_]\n",
    "    show_img.shape = [28, 28]\n",
    "    plt.subplot(4, 8, _ + 1)\n",
    "    plt.imshow(show_img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个核还是主要学习到了边缘特征，第五个核还是学到了骨干特征（当然在某种程度上）。好吧，本次就到这啦！"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
